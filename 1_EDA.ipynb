{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Resources: [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008)\n",
    "\n",
    "[Kaggle Link](https://www.kaggle.com/pavan2029/diabetic-data)\n",
    "\n",
    "**Objective:**\n",
    "Hospital readmission rates for certain conditions are now considered an indicator of hospital quality, and also affect the cost of care adversely. Hospital readmissions of diabetic patients are expensive as hospitals face penalties if their readmission rate is higher than expected and reflects the inadequacies in health care system. For these reasons, it is important for the hospitals to improve focus on reducing readmission rates. Identify the key factors that influence readmission for diabetes and to predict the probability of patient readmission. \n",
    "\n",
    "The dataset represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. The data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('diabetic_data.zip').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv contains \"?\" for missing values. We replace it with NaN\n",
    "data = pd.read_csv('diabetic_data.zip', na_values=[\"?\"])\n",
    "df= data.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Features Dataset\n",
    "Descriptions of the features:\n",
    "https://www.hindawi.com/journals/bmri/2014/781670/tab1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('features.csv',index_col='Unnamed: 0')\n",
    "info = lambda attribute:print(f\"{attribute.upper()} : {features[features['Feature']==attribute]['Description'].values[0]}\\n\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info('encounter_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().value_counts()\n",
    "# df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> no duplicates detected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df, pred=None):\n",
    "    obs = df.shape[0]\n",
    "    Types = df.dtypes\n",
    "    Counts = df.apply(lambda x: x.count())\n",
    "    Min = df.min()\n",
    "    Max = df.max()\n",
    "    Uniques = df.apply(lambda x: x.unique().shape[0])\n",
    "    Nulls = df.apply(lambda x: x.isnull().sum())\n",
    "    print('Data shape:', df.shape)\n",
    "\n",
    "    if pred is None:\n",
    "        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n",
    "        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n",
    "\n",
    "    str.columns = cols\n",
    "    print('___________________________\\nData Types:')\n",
    "    print(str.Types.value_counts())\n",
    "    print('___________________________')\n",
    "    return str\n",
    "\n",
    "display(summary(df).sort_values(by='Nulls', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* 'citoglipton' and 'examide' features that the number of uniques is 1 are droped.\n",
    "* all values of 'encounter_id' column are unique. It has to be droped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['citoglipton','examide','encounter_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOCUS ON \"Gender\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> We regard the observations of \"Unknown/Invalid\" gender as null values and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_index = df[df.gender == 'Unknown/Invalid'].index\n",
    "df = df.drop(gender_index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm removal\n",
    "df.gender.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOCUS ON \"readmitted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.readmitted.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Patients readmitted to the hospital within and after 30 days will be combined into one column, because these patients ultimately returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(['<30', '>30'], 'YES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOCUS  ON \" patient_nbr \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info('patient_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['patient_nbr'].duplicated().value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* we can think of 'patient_nbr' as the id number of each patient.\n",
    "* It turned out that the dataset is the data of 71515 unique patients.\n",
    "* Some patients visited the hospital multiple times for treatment so to avoid over-representing any particular individual, only the first encounter with a patient will be used / kept in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total unique patients\n",
    "len(df.patient_nbr), df.patient_nbr.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate number of patient visits using patient_id\n",
    "df.patient_nbr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only one record for each patient, the first visit\n",
    "df = df.drop_duplicates(['patient_nbr'], keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.patient_nbr.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* Since patient_nbr is unique, it is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('patient_nbr', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def null_values(df):\n",
    "    \"\"\"a function to show null values with percentage\"\"\"\n",
    "    nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n",
    "    return nv[nv.Missing_Records>0].sort_values('Missing_Records', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with missing values\n",
    "null_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['weight','medical_specialty','payer_code']: info(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* The majority of patients do not have a weight listed so this column can be dropped. \n",
    "* Medical specialty and payer code are also missing for about half of the patients. \n",
    "* We do not need to know how the patients paid for their treatments.\n",
    "* we do not have enough information to figure out which medical unit they went to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['weight','medical_specialty','payer_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(df).sort_values(by='Uniques', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']: info(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* We dont need 'admission_type_id', 'discharge_disposition_id', 'admission_source_id' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "drop_cols = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "df = df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.race.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* Since there is no way to know the race of the patient using existing information, the best option is to remove those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, subset=['race'])\n",
    "null_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['diag_1', 'diag_2', 'diag_3']: info(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* You can reach the extensive diagnosis description on this website by querying with the ICD9 code:\n",
    "http://icd9.chrisendres.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we are down to three columns with missing information: diagnosis 1, 2, and 3. \n",
    "* Diagnosis 1 is described as the primary diagnosis made during the patient's visit while diagnosis 2 is the second and 3 is an any additional diagnoses made after that. \n",
    "* Looking at the patients' rows that are missing a primary diagnosis, most of them have a second diagnosis or even a third. \n",
    "* Since it doesn't make sense to have a second (or third) but not a primary diagnosis, we will remove these columns from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info('number_diagnoses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['diag_1', 'diag_2', 'diag_3','number_diagnoses']][df.diag_1.isnull() & df.diag_2.notnull() & df.diag_3.notnull() & df.number_diagnoses.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The number of diagnoses column shows the total number of conditions a patient is diagnosed with. Only the first three are recorded, so those that are missing the first diagnosis but still a second or third are in error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where diagnosis 1 is missing\n",
    "df = df.dropna(axis=0, subset=['diag_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There are two remaining diagnosis columns with missing values. Each number correlates to a specific condition so if there is a missing value, then it is likely that the patient only has one diagnosed condition. The number of diagnoses column lists the total number of diagnosed conditions. When looking at all three diagnosis columns, if the number is one, then diagnosis 2 and 3 can be filled in with a 0 to show that there is no additional diagnosis. If diagnosis 2 or 3 is missing a value and the number of diagnoses is greater than one, then some diagnoses were not recorded and the rows should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['diag_1','diag_2', 'diag_3','number_diagnoses']][df.diag_2.isnull() & (df.diag_3.notnull()|(df.number_diagnoses > 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where diagnosis 2 is missing and number of diagnoses is greater than 1\n",
    "diag_2_indexes = df[df.diag_2.isnull() & (df.diag_3.notnull()|(df.number_diagnoses > 1))].index\n",
    "df = df.drop(index = diag_2_indexes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diagnosis 3 is the last column left with unaccounted missing values. Since some patients have 1 or 2 diagnosed conditions, the diagnosis 3 column is left intentionally blank. The goal here is to remove the rows that have a diagnoses number greater than two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of affected rows\n",
    "df[['diag_1','diag_2', 'diag_3', 'number_diagnoses']][df.diag_3.isnull() & (df.number_diagnoses > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing diagnosis 3 and number of diagnoses is greater than 2\n",
    "diag_3_indexes = df[(df.diag_3.isnull()) & (df.number_diagnoses > 2)].index\n",
    "df = df.drop(index=diag_3_indexes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[['diag_1','diag_2', 'diag_3','number_diagnoses']].isnull(),yticklabels=False,cbar=False,cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with None in diagnosis 2 and 3 to show there is no additional diagnosis\n",
    "df.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm there are no more NaN values\n",
    "null_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Diagnosis Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(df[['diag_1','diag_2', 'diag_3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* 'diag_1','diag_2' and 'diag_3' columns contain codes for the types of conditions patients are diagnosed with. \n",
    "* There are too much unique codes throughout this dataset.\n",
    "* We can group the related icd9 diagnosis codes among themselves. In this way, we use categorical group names instead of numerical codes.\n",
    "* The grouping is based on the research paper table (https://www.hindawi.com/journals/bmri/2014/781670/tab2/)\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Group Names**\n",
    "\n",
    "    1-Circulatory\n",
    "    2-Respiratory\n",
    "    3-Digestive\n",
    "    4-Diabetes\n",
    "    5-Injury\n",
    "    6-Musculoskeletal\n",
    "    7-Genitourinary\n",
    "    8-Neoplasms\n",
    "    9-Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circulatory\n",
    "codes =[str(i) for i in list(range(390,460)) + [785]]\n",
    "df = df.replace(codes, 'Circulatory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respiratory\n",
    "codes =[str(i) for i in list(range(460,520)) + [786]]\n",
    "df = df.replace(codes, 'Respiratory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digestive\n",
    "codes =[str(i) for i in list(range(520,580)) + [787]]\n",
    "df = df.replace(codes, 'Digestive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes\n",
    "df = df.replace(regex=r'^250.*', value='Diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Injury\n",
    "codes =[str(i) for i in range(800,1000)]\n",
    "df = df.replace(codes, 'Injury')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Musculoskeletal\n",
    "codes =[str(i) for i in range(710,740)]\n",
    "df = df.replace(codes, 'Musculoskeletal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genitourinary\n",
    "codes =[str(i) for i in list(range(580,630)) + [788]]\n",
    "df = df.replace(codes, 'Genitourinary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neoplasms\n",
    "codes =[str(i) for i in range(140,240)]\n",
    "df = df.replace(codes, 'Neoplasms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other\n",
    "df = df.replace(regex=r'^[E,V].*', value='Other')\n",
    "\n",
    "codes =[str(i) for i in range(0,1000)]\n",
    "df = df.replace(codes, 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['diag_1', 'diag_2', 'diag_3']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values of Each Features:\n",
    "for i in df[['diag_1', 'diag_2', 'diag_3']]:\n",
    "    print(f'{i}:\\n{sorted(df[i].unique())}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add 365.44 to Other\n",
    "df = df.replace('365.44', 'Other') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "for diag in ['diag_1','diag_2','diag_3']:\n",
    "    sns.lineplot(x=df[diag].value_counts().sort_index().index, y= df[diag].value_counts().sort_index().values, marker='o')\n",
    "plt.legend(['diag_1','diag_2','diag_3'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* Looking at the graph above, we can say that there is a high correlation between the diagnoses. So we drop diag_2 and diag_3.\n",
    "* Also since the most common diagnoses are prevalent in all three diagnoses listed, We are only using the primary diagnosis variable to build the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop diagnoses 2 and 3\n",
    "df = df.drop(columns=['diag_2', 'diag_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOCUS ON \"`number_diagnoses`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "ax = df.number_diagnoses.value_counts().sort_index().plot.bar()\n",
    "def labels(ax, df=df):\n",
    "    for p in ax.patches:\n",
    "            ax.annotate('{:.0f}'.format(p.get_height()), \n",
    "                        (p.get_x(), p.get_height()+100),size=10)\n",
    "labels(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* For a small number of observations with number of diagnoses greater than 9, let's change the number of diagnoses to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.number_diagnoses = df.number_diagnoses.replace([10,11,12,13,14,15,16],9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Based on the basic statistics describing the dataset, it looks there are outliers that influence skewness in the data. In order to represent the majority of samples and build clean models, we are going to remove outliers that have [z-scores](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/) greater than 3.0 or less than -3.0. This means that we are removing samples that are more (or less) than 3 times the standard deviation from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.describe().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_plot(df,col_name):\n",
    "    plt.figure(figsize=(15,6))\n",
    "    \n",
    "    plt.subplot(141) # 1 satir x 4 sutun dan olusan ax in 1. sutununda calis\n",
    "    plt.hist(df[col_name], bins = 20)\n",
    "    f=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\n",
    "    \n",
    "    # üç sigma aralikta(verinin %99.7 sini icine almasi beklenen bolum) iki kirmizi cizgi arasinda\n",
    "    plt.axvline(x=df[col_name].mean() + 3*df[col_name].std(),color='red')\n",
    "    plt.axvline(x=df[col_name].mean() - 3*df[col_name].std(),color='red')\n",
    "    plt.xlabel(col_name)\n",
    "    plt.tight_layout\n",
    "    plt.xlabel(\"Histogram ±3z\")\n",
    "    plt.ylabel(col_name)\n",
    "\n",
    "    plt.subplot(142)\n",
    "    plt.boxplot(df[col_name]) # IQR katsayisi, defaultu 1.5\n",
    "    plt.xlabel(\"IQR=1.5\")\n",
    "\n",
    "    plt.subplot(143)\n",
    "    plt.boxplot(df[col_name].apply(f), whis = 2.5)\n",
    "    plt.xlabel(\"ROOT SQUARE - IQR=2.5\")\n",
    "\n",
    "    plt.subplot(144)\n",
    "    plt.boxplot(np.log(df[col_name]+0.1), whis = 2.5)\n",
    "    plt.xlabel(\"LOGARITMIC - IQR=2.5\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in features:\n",
    "    col_plot(df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "def plot_winsorize(df,col_name,up=0.1,down=0):\n",
    "    plt.figure(figsize = (15, 6))\n",
    "\n",
    "    winsor=winsorize(df[col_name], (down,up))\n",
    "    logr=np.log(df[col_name]+0.1)\n",
    "\n",
    "    plt.subplot(141)\n",
    "    plt.hist(winsor, bins = 22)\n",
    "    plt.axvline(x=winsor.mean()+3*winsor.std(),color='red')\n",
    "    plt.axvline(x=winsor.mean()-3*winsor.std(),color='red')\n",
    "    plt.xlabel('Winsorize_Histogram')\n",
    "    plt.ylabel(col_name)\n",
    "    plt.tight_layout\n",
    "\n",
    "    plt.subplot(142)\n",
    "    plt.boxplot(winsor, whis = 1.5)\n",
    "    plt.xlabel('Winsorize - IQR:1.5')\n",
    "    \n",
    "    plt.subplot(143)\n",
    "    plt.hist(logr, bins=22)\n",
    "    plt.axvline(x=logr.mean()+3*logr.std(),color='red')\n",
    "    plt.axvline(x=logr.mean()-3*logr.std(),color='red')\n",
    "    plt.xlabel('Logr_col_name')\n",
    "\n",
    "    plt.subplot(144)\n",
    "    plt.boxplot(logr, whis = 1.5)\n",
    "    plt.xlabel(\"Logaritmic - IQR=1.5\")\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in features:\n",
    "    plot_winsorize(df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_winsorised=df.copy()\n",
    "for i in features:\n",
    "    df_winsorised[i]=winsorize(df_winsorised[i], (0,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log=df.copy()\n",
    "for i in features:\n",
    "    df_log[i]=np.log(df_log[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_root=df.copy()\n",
    "f=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\n",
    "for i in features:\n",
    "    df_root[i]=df_root[i].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import percentile\n",
    "from scipy.stats import zscore\n",
    "from scipy import stats\n",
    "\n",
    "def outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.1, print_list = False):\n",
    "    z_scores = zscore(df[col].dropna())\n",
    "    threshold_list = []\n",
    "    for threshold in np.arange(min_z, max_z, step):\n",
    "        threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n",
    "        df_outlier = pd.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n",
    "        df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))/df_outlier.outlier_count*100\n",
    "    plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n",
    "    best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n",
    "    outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n",
    "    percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n",
    "    plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), \n",
    "               colors=\"r\", ls = \":\"\n",
    "              )\n",
    "    plt.annotate(\"Zscore : {}\\nValue : {}\\nPercentile : {}\".format(best_treshold, outlier_limit, \n",
    "                                                                   (np.round(percentile_threshold, 3), \n",
    "                                                                    np.round(100-percentile_threshold, 3))), \n",
    "                 (best_treshold, df_outlier.outlier_count.max()/2))\n",
    "    #plt.show()\n",
    "    if print_list:\n",
    "        print(df_outlier)\n",
    "    return (plt, df_outlier, best_treshold, outlier_limit, percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from scipy import stats\n",
    "\n",
    "def outlier_inspect(df, col, min_z=1, max_z = 5, step = 0.5, max_hist = None, bins = 50):\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    fig.suptitle(col, fontsize=16)\n",
    "    plt.subplot(1,3,1)\n",
    "    if max_hist == None:\n",
    "        sns.distplot(df[col], kde=False, bins = 50)\n",
    "    else :\n",
    "        sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n",
    "   \n",
    "    plt.subplot(1,3,2)\n",
    "    sns.boxplot(df[col])\n",
    "    plt.subplot(1,3,3)\n",
    "    z_score_inspect = outlier_zscore(df, col, min_z=min_z, max_z = max_z, step = step)\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.axvline(x=df[col].mean() + z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n",
    "    plt.axvline(x=df[col].mean() - z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n",
    "    plt.show()\n",
    "    \n",
    "    return z_score_inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df:pd.DataFrame, col_name:str, p=1.5) ->int:\n",
    "    ''' \n",
    "    this function detects outliers based on 3 time IQR and\n",
    "    returns the number of lower and uper limit and number of outliers respectively\n",
    "    '''\n",
    "    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                      \n",
    "    upper_limit = third_quartile+(p*IQR)\n",
    "    lower_limit = first_quartile-(p*IQR)\n",
    "    outlier_count = 0\n",
    "                      \n",
    "    for value in df[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "    return lower_limit, upper_limit, outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "print(f\"Number of Outliers for {k}*IQR\\n\")\n",
    "\n",
    "total=0\n",
    "for col in features:\n",
    "    if detect_outliers(df, col)[2] > 0:\n",
    "        outliers=detect_outliers(df, col, k)[2]\n",
    "        total+=outliers\n",
    "        print(\"{} outliers in '{}'\".format(outliers,col))\n",
    "print(\"\\n{} OUTLIERS TOTALLY\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "print(f\"Number of Outliers for {k}*IQR after Root Square\\n\")\n",
    "\n",
    "total=0\n",
    "for col in features:\n",
    "    if detect_outliers(df_root, col)[2] > 0:\n",
    "        outliers=detect_outliers(df_root, col, k)[2]\n",
    "        total+=outliers\n",
    "        print(\"{} outliers in '{}'\".format(outliers,col))\n",
    "print(\"\\n{} OUTLIERS TOTALLY\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "print(f\"Number of Outliers for {k}*IQR after Winsorised\\n\")\n",
    "\n",
    "total=0\n",
    "for col in features:\n",
    "    if detect_outliers(df_winsorised, col)[2] > 0:\n",
    "        outliers=detect_outliers(df_winsorised, col, k)[2]\n",
    "        total+=outliers\n",
    "        print(\"{} outliers in '{}'\".format(outliers,col))\n",
    "print(\"\\n{} OUTLIERS TOTALLY\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "print(f\"Number of Outliers for {k}*IQR after Logarithmed\\n\")\n",
    "\n",
    "total=0\n",
    "for col in features:\n",
    "    if detect_outliers(df_log, col)[2] > 0:\n",
    "        outliers=detect_outliers(df_log, col, k)[2]\n",
    "        total+=outliers\n",
    "        print(\"{} outliers in '{}'\".format(outliers,col))\n",
    "print(\"\\n{} OUTLIERS TOTALLY\".format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores=[]\n",
    "for i in features:\n",
    "    z_scores.append(outlier_inspect(df,i)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 1\n",
    "# df_3z=df.copy()\n",
    "# for i in features:\n",
    "#     down_limit= df_3z[i].mean() - (3*df_3z[i].std())\n",
    "#     upper_limit= df_3z[i].mean() + (3*df_3z[i].std())\n",
    "#     condition= (df_3z[i] > down_limit) & (df_3z[i] < upper_limit)\n",
    "#     df_3z=df_3z[condition]\n",
    "\n",
    "# print('Number of Outliers:',len(df)-len(df_3z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 2\n",
    "\n",
    "# create columns for z scores, new column with z score\n",
    "df_3z=df.copy()\n",
    "\n",
    "for x in features:\n",
    "    df_3z[x + '_z'] = stats.zscore(df_3z[x])\n",
    "\n",
    "for x in df_3z.columns[-len(features):]:\n",
    "    df_3z = df_3z[(df_3z[x] < 3) & (df_3z[x] > -3)]\n",
    "    \n",
    "# drop _z columns\n",
    "df_3z = df_3z.drop(columns=df_3z.columns[-8:])\n",
    "\n",
    "print('Number of Outliers:',len(df)-len(df_3z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3z.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Investigate the unique values of each column and look for error entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(df_3z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Drop the columns that the number of uniques is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3z = df_3z.drop(['acetohexamide','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3z = df_3z.reset_index(drop=True)\n",
    "df_3z.to_csv('diabetic_data_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
